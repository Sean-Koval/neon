{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Neon SDK\n",
    "\n",
    "This notebook demonstrates the basic features of the Neon Python SDK for agent evaluation.\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install neon-sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the SDK if not already installed\n",
    "# !pip install neon-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Tracing\n",
    "\n",
    "The SDK provides context managers for tracing agent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.tracing import trace, span, generation, tool\n",
    "\n",
    "# Simple trace with nested spans\n",
    "with trace(\"my-agent\", metadata={\"version\": \"1.0\"}):\n",
    "    print(\"Starting agent...\")\n",
    "    \n",
    "    with span(\"preprocessing\"):\n",
    "        data = \"Hello, World!\"\n",
    "        print(f\"Preprocessed: {data}\")\n",
    "    \n",
    "    with generation(\"llm-call\", model=\"gpt-4\"):\n",
    "        # Simulated LLM call\n",
    "        response = f\"Processed: {data}\"\n",
    "        print(f\"LLM Response: {response}\")\n",
    "    \n",
    "    with tool(\"calculator\", tool_name=\"math\"):\n",
    "        result = 2 + 2\n",
    "        print(f\"Tool Result: {result}\")\n",
    "\n",
    "print(\"Agent completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using the Decorator\n",
    "\n",
    "For simpler cases, use the `@traced` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.tracing import traced\n",
    "import asyncio\n",
    "\n",
    "@traced(\"sync-function\")\n",
    "def process_sync(x: int) -> int:\n",
    "    \"\"\"A traced synchronous function.\"\"\"\n",
    "    return x * 2\n",
    "\n",
    "@traced(\"async-function\")\n",
    "async def process_async(x: int) -> int:\n",
    "    \"\"\"A traced asynchronous function.\"\"\"\n",
    "    await asyncio.sleep(0.1)\n",
    "    return x * 3\n",
    "\n",
    "# Test sync function\n",
    "result1 = process_sync(5)\n",
    "print(f\"Sync result: {result1}\")\n",
    "\n",
    "# Test async function\n",
    "result2 = await process_async(5)\n",
    "print(f\"Async result: {result2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rule-Based Scorers\n",
    "\n",
    "Scorers evaluate agent performance. Let's start with simple rule-based scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import contains, exact_match, ContainsConfig, ExactMatchConfig\n",
    "from neon_sdk.scorers.base import EvalContext\n",
    "\n",
    "# Contains scorer - check if output contains specific strings\n",
    "contains_scorer = contains([\"hello\", \"world\"])\n",
    "\n",
    "result = contains_scorer.evaluate(EvalContext(\n",
    "    output=\"Hello, World! This is a test.\",\n",
    "    input={\"query\": \"greeting\"},\n",
    "))\n",
    "\n",
    "print(f\"Contains Score: {result.value}\")\n",
    "print(f\"Reason: {result.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match scorer with configuration\n",
    "exact_scorer = exact_match(ExactMatchConfig(\n",
    "    expected=\"Hello World\",\n",
    "    case_sensitive=False,\n",
    "    normalize_whitespace=True,\n",
    "))\n",
    "\n",
    "result = exact_scorer.evaluate(EvalContext(\n",
    "    output=\"  hello   world  \",\n",
    "))\n",
    "\n",
    "print(f\"Exact Match Score: {result.value}\")\n",
    "print(f\"Reason: {result.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Scorers\n",
    "\n",
    "Define your own scorers for domain-specific evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import scorer, ScoreResult\n",
    "\n",
    "@scorer(\"word_count\")\n",
    "def word_count_scorer(context: EvalContext) -> ScoreResult:\n",
    "    \"\"\"Score based on word count (normalized to 0-1).\"\"\"\n",
    "    words = context.output.split() if context.output else []\n",
    "    count = len(words)\n",
    "    # Normalize: 100+ words = 1.0\n",
    "    score = min(count / 100, 1.0)\n",
    "    \n",
    "    return ScoreResult(\n",
    "        value=score,\n",
    "        reason=f\"Word count: {count} ({score:.2%} of target)\",\n",
    "    )\n",
    "\n",
    "# Test the custom scorer\n",
    "result = word_count_scorer.evaluate(EvalContext(\n",
    "    output=\"This is a sample response with some words. \" * 5,\n",
    "))\n",
    "\n",
    "print(f\"Word Count Score: {result.value:.2f}\")\n",
    "print(f\"Reason: {result.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting It Together\n",
    "\n",
    "Let's create a simple agent and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.tracing import trace, generation\n",
    "from neon_sdk.scorers import contains\n",
    "from neon_sdk.scorers.base import EvalContext\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": {\"query\": \"What is Python?\"},\n",
    "        \"expected\": [\"programming\", \"language\"],\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"query\": \"What is machine learning?\"},\n",
    "        \"expected\": [\"algorithms\", \"data\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Simulated agent function\n",
    "def simple_agent(query: str) -> str:\n",
    "    \"\"\"A simple agent that returns predefined responses.\"\"\"\n",
    "    responses = {\n",
    "        \"What is Python?\": \"Python is a high-level programming language known for its simplicity.\",\n",
    "        \"What is machine learning?\": \"Machine learning uses algorithms to learn patterns from data.\",\n",
    "    }\n",
    "    return responses.get(query, \"I don't know.\")\n",
    "\n",
    "# Run evaluation\n",
    "results = []\n",
    "\n",
    "for case in test_cases:\n",
    "    query = case[\"input\"][\"query\"]\n",
    "    expected = case[\"expected\"]\n",
    "    \n",
    "    # Trace the agent\n",
    "    with trace(\"simple-agent\", input=case[\"input\"]):\n",
    "        with generation(\"response\", model=\"simple-v1\"):\n",
    "            response = simple_agent(query)\n",
    "    \n",
    "    # Score the response\n",
    "    scorer = contains(expected)\n",
    "    score = scorer.evaluate(EvalContext(\n",
    "        input=case[\"input\"],\n",
    "        output=response,\n",
    "    ))\n",
    "    \n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"score\": score.value,\n",
    "        \"reason\": score.reason,\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"Query: {r['query']}\")\n",
    "    print(f\"Response: {r['response']}\")\n",
    "    print(f\"Score: {r['score']:.2f}\")\n",
    "    print(f\"Reason: {r['reason']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "print(f\"\\nAverage Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [02_advanced_scorers.ipynb](02_advanced_scorers.ipynb) - LLM judges and causal analysis\n",
    "- [03_clickhouse_analytics.ipynb](03_clickhouse_analytics.ipynb) - Trace storage and analytics\n",
    "- [04_temporal_workflows.ipynb](04_temporal_workflows.ipynb) - Durable workflow execution\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Documentation](https://neon-sdk.readthedocs.io)\n",
    "- [GitHub Repository](https://github.com/neon-dev/neon)\n",
    "- [API Reference](https://neon-sdk.readthedocs.io/en/latest/api/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
