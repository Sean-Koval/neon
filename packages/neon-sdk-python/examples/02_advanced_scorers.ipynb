{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Scorers\n",
    "\n",
    "This notebook covers advanced scoring techniques including LLM judges and causal analysis.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install neon-sdk\n",
    "```\n",
    "\n",
    "You'll need an Anthropic API key for the LLM judge examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set your API key (or use environment variable)\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM Judge Scorers\n",
    "\n",
    "LLM judges use a language model to evaluate agent responses with semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import llm_judge, LLMJudgeConfig\n",
    "from neon_sdk.scorers.base import EvalContext\n",
    "\n",
    "# Create a custom LLM judge\n",
    "quality_judge = llm_judge(LLMJudgeConfig(\n",
    "    prompt='''Evaluate the quality of this response on a scale of 0 to 1.\n",
    "\n",
    "User Query: {{input}}\n",
    "Agent Response: {{output}}\n",
    "\n",
    "Consider:\n",
    "- Accuracy: Is the information correct?\n",
    "- Completeness: Does it fully answer the question?\n",
    "- Clarity: Is it easy to understand?\n",
    "\n",
    "Return your evaluation as JSON:\n",
    "{\"score\": <0-1>, \"reason\": \"<brief explanation>\"}''',\n",
    "    model='claude-3-haiku-20240307',\n",
    "))\n",
    "\n",
    "# Note: This requires an API key to run\n",
    "# result = await quality_judge.evaluate(EvalContext(\n",
    "#     input={\"query\": \"What is Python?\"},\n",
    "#     output=\"Python is a programming language.\",\n",
    "# ))\n",
    "# print(f\"Score: {result.value}\")\n",
    "# print(f\"Reason: {result.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-Built Judges\n",
    "\n",
    "The SDK includes pre-configured judges for common evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import (\n",
    "    response_quality_judge,\n",
    "    safety_judge,\n",
    "    helpfulness_judge,\n",
    ")\n",
    "\n",
    "# These are ready to use\n",
    "print(\"Pre-built judges available:\")\n",
    "print(\"- response_quality_judge: Evaluates overall response quality\")\n",
    "print(\"- safety_judge: Checks for harmful or unsafe content\")\n",
    "print(\"- helpfulness_judge: Measures how helpful the response is\")\n",
    "\n",
    "# Example usage (requires API key):\n",
    "# result = await response_quality_judge.evaluate(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Response Parser\n",
    "\n",
    "You can customize how LLM responses are parsed into scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple yes/no judge\n",
    "yes_no_judge = llm_judge(LLMJudgeConfig(\n",
    "    prompt='''Is this response helpful to the user?\n",
    "\n",
    "Response: {{output}}\n",
    "\n",
    "Answer with only YES or NO.''',\n",
    "    model='claude-3-haiku-20240307',\n",
    "    parse_response=lambda text: 1.0 if 'YES' in text.upper() else 0.0,\n",
    "))\n",
    "\n",
    "print(\"Created yes/no judge with custom parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Causal Analysis Scorers\n",
    "\n",
    "Analyze error propagation and identify root causes in agent traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import (\n",
    "    causal_analysis_scorer,\n",
    "    CausalAnalysisConfig,\n",
    "    analyze_causality,\n",
    ")\n",
    "\n",
    "# Create a causal analysis scorer with custom weights\n",
    "causal_scorer = causal_analysis_scorer(CausalAnalysisConfig(\n",
    "    root_cause_weight=0.6,      # Weight for root cause identification\n",
    "    chain_completeness_weight=0.3,  # Weight for causal chain completeness\n",
    "    error_rate_weight=0.1,      # Weight for overall error rate\n",
    "))\n",
    "\n",
    "print(\"Causal analysis scorer created\")\n",
    "print(\"This scorer analyzes traces to identify:\")\n",
    "print(\"- Root causes of failures\")\n",
    "print(\"- Error propagation chains\")\n",
    "print(\"- Component dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Composite Scorers\n",
    "\n",
    "Combine multiple scorers with weighted averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import contains, latency_scorer, LatencyThresholds\n",
    "from neon_sdk.scorers.base import EvalContext, ScoreResult\n",
    "\n",
    "# Define individual scorers\n",
    "keyword_scorer = contains([\"helpful\", \"thank\"])\n",
    "speed_scorer = latency_scorer(LatencyThresholds(\n",
    "    excellent=500,\n",
    "    good=2000,\n",
    "    acceptable=5000,\n",
    "))\n",
    "\n",
    "# Create a composite scorer\n",
    "def composite_evaluate(context: EvalContext) -> ScoreResult:\n",
    "    \"\"\"Combine multiple scorers with weights.\"\"\"\n",
    "    scores = [\n",
    "        (keyword_scorer.evaluate(context), 0.4),\n",
    "        # speed_scorer requires trace data\n",
    "    ]\n",
    "    \n",
    "    weighted_sum = sum(s.value * w for s, w in scores)\n",
    "    total_weight = sum(w for _, w in scores)\n",
    "    \n",
    "    return ScoreResult(\n",
    "        value=weighted_sum / total_weight,\n",
    "        reason=\"Composite score from multiple criteria\",\n",
    "    )\n",
    "\n",
    "# Test\n",
    "result = composite_evaluate(EvalContext(\n",
    "    output=\"Thank you for your helpful question!\",\n",
    "))\n",
    "print(f\"Composite Score: {result.value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scorer Metadata\n",
    "\n",
    "Add metadata to scores for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import scorer, ScoreResult, EvalContext\n",
    "\n",
    "@scorer(\"detailed_analysis\")\n",
    "def detailed_scorer(context: EvalContext) -> ScoreResult:\n",
    "    \"\"\"Scorer that includes detailed metadata.\"\"\"\n",
    "    output = context.output or \"\"\n",
    "    \n",
    "    # Analyze various aspects\n",
    "    word_count = len(output.split())\n",
    "    sentence_count = output.count('.') + output.count('!') + output.count('?')\n",
    "    avg_word_length = sum(len(w) for w in output.split()) / max(word_count, 1)\n",
    "    \n",
    "    # Calculate composite score\n",
    "    length_score = min(word_count / 50, 1.0)\n",
    "    \n",
    "    return ScoreResult(\n",
    "        value=length_score,\n",
    "        reason=f\"Based on {word_count} words in {sentence_count} sentences\",\n",
    "        metadata={\n",
    "            \"word_count\": word_count,\n",
    "            \"sentence_count\": sentence_count,\n",
    "            \"avg_word_length\": round(avg_word_length, 2),\n",
    "            \"length_score\": round(length_score, 2),\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Test\n",
    "result = detailed_scorer.evaluate(EvalContext(\n",
    "    output=\"This is a sample response. It contains multiple sentences. The scorer analyzes various aspects of the text to produce a comprehensive evaluation.\",\n",
    "))\n",
    "\n",
    "print(f\"Score: {result.value:.2f}\")\n",
    "print(f\"Reason: {result.reason}\")\n",
    "print(\"Metadata:\")\n",
    "for key, value in result.metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Async Scorers\n",
    "\n",
    "Create async scorers for external API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neon_sdk.scorers import scorer, ScoreResult, EvalContext\n",
    "import asyncio\n",
    "\n",
    "@scorer(\"async_external\")\n",
    "async def async_scorer(context: EvalContext) -> ScoreResult:\n",
    "    \"\"\"Async scorer that could call external APIs.\"\"\"\n",
    "    # Simulate async API call\n",
    "    await asyncio.sleep(0.1)\n",
    "    \n",
    "    # Process result\n",
    "    output = context.output or \"\"\n",
    "    score = 1.0 if len(output) > 10 else 0.5\n",
    "    \n",
    "    return ScoreResult(\n",
    "        value=score,\n",
    "        reason=\"Evaluated via async process\",\n",
    "    )\n",
    "\n",
    "# Test\n",
    "result = await async_scorer.evaluate(EvalContext(\n",
    "    output=\"This is a longer response that should score well.\",\n",
    "))\n",
    "print(f\"Async Score: {result.value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [03_clickhouse_analytics.ipynb](03_clickhouse_analytics.ipynb) - Store and query traces\n",
    "- [04_temporal_workflows.ipynb](04_temporal_workflows.ipynb) - Durable workflow execution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
